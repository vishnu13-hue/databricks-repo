{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75cf95e8-cc0b-40bc-8134-a56cb67a6d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS dev_catalog.default.raw_json_data\n",
    "COMMENT \"Volume for raw JSON data storage\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45ff49bb-7d1c-48d3-9c8c-c358a12f7ffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "921e4bf7-79c6-4086-a293-e628942420bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create raw data directory\n",
    "#dbutils.fs.mkdirs(\"dbfs:/Volumes/dev_catalog/default/raw_json_data/landing/raw_datas\")\n",
    "# Create checkpoint directory for bronze layer\n",
    "#dbutils.fs.mkdirs(\"dbfs:/Volumes/dev_catalog/default/raw_json_data/checkpoints/customer/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e5acab9-a274-4811-adff-9696f3b620b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.fs.mkdirs(\"dbfs:/Volumes/dev_catalog/default/raw_json_data/checkpoints/schema/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a4c209c-b2a5-4462-995b-1c8ab3f4c8ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Bronze Notebook (bronze_autoloader_customer.py)\n",
    "# ---------------------------------------------\n",
    "from pyspark.sql.functions import input_file_name, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "# Catalog + Schema + Table setup\n",
    "catalog = \"dev_catalog\"\n",
    "schema = \"default\"\n",
    "table = \"bronze_customer\"\n",
    "\n",
    "# Use the paths provided by the user\n",
    "input_path = \"/Volumes/dev_catalog/default/raw_json_data/landing/raw/\"\n",
    "checkpoint_path = \"/Volumes/dev_catalog/default/raw_json_data/checkpoints/customer/\"\n",
    "schema_path = \"/Volumes/dev_catalog/default/raw_json_data/checkpoints/schema/\"\n",
    "\n",
    "# Define static schema for nested JSON parsing\n",
    "customer_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"operation\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip_code\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read JSON files using Auto Loader with schema\n",
    "from pyspark.sql.functions import current_timestamp,col\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "        .schema(customer_schema)\n",
    "        .load(input_path)\n",
    "        .withColumn(\"ingest_time\", current_timestamp())\n",
    "        .withColumn(\"source_file\", col(\"_metadata.file_path\")) # âœ… Correct way\n",
    ")\n",
    "\n",
    "\n",
    "# Write to Bronze Delta Table\n",
    "df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(availableNow=True).toTable(f\"{catalog}.{schema}.{table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "917d8c05-f21d-4aaa-8900-5b9824f3c854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from dev_catalog.default.bronze_customer"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7883332051497091,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Auto_loader - bronze layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
